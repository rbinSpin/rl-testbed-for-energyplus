# 模擬實驗的檔案細節

我的實驗主要更動兩個腳本檔案

1. 我更動了 `rl-testbed-for-energyplus//baselines_energyplus/trpo_mpi` 中的 `run_energyplus.py` 
2. 以及我新加入一樣在 `rl-testbed-for-energyplus//baselines_energyplus/trpo_mpi` 中的  `simulation_episodes.py`

# `run_energyplus.py`

新添加了 `ModelSaver` 物件

```python
class ModelSaver:
    def __init__(self, save_path, save_frequency=1):
        self.save_path = save_path
        self.save_frequency = save_frequency
        self.iteration = 0
        self.start_time = time.time()

    def __call__(self, locals, globals):
        self.iteration += 1
        if self.iteration % self.save_frequency == 0:
            model = locals.get('pi')
            if model and hasattr(model, 'save'):
                save_path = os.path.join(self.save_path, f'model_iter_{self.iteration}')
                model.save(save_path)
                print(f"Model saved at iteration {self.iteration} in path: {save_path}")

        # 每小時印一次進度
        if time.time() - self.start_time > 3600:
            self.start_time = time.time()
            print(f"Training progress: iteration {self.iteration}")
```

在訓練時，使用回調函數來儲存模型

```python
# 創建 ModelSaver 實例
model_saver = ModelSaver(save_path=models_dir, save_frequency=1)

def combined_callback(locals, globals):
    model_saver(locals, globals)
    # 在這裡可以添加其他回調函數

pi = trpo_mpi.learn(env=env,
                    network=mlp(num_hidden=32, num_layers=2),
                    total_timesteps=num_timesteps,
                    timesteps_per_batch=16*1024,
                    max_kl=0.01,
                    cg_iters=10,
                    cg_damping=0.1,
                    gamma=0.99,
                    lam=0.98,
                    vf_iters=5,
                    vf_stepsize=1e-3,
                    callback=combined_callback)

# 再次儲存，以確保有存到最後的模型
pi.save(f"{models_dir}/final")

```

其中，

```python
pi = trpo_mpi.learn(env=env,
                    network=mlp(num_hidden=32, num_layers=2),
                    total_timesteps=num_timesteps,
                    timesteps_per_batch=16*1024,
                    max_kl=0.01,
                    cg_iters=10,
                    cg_damping=0.1,
                    gamma=0.99,
                    lam=0.98,
                    vf_iters=5,
                    vf_stepsize=1e-3,
                    callback=combined_callback)
```

包含了主要的訓練過程，而模型參數就存在 pi 這個物件裡。

## 批次（batch）訓練

注意上面 model_saver

```python
# 創建 ModelSaver 實例
model_saver = ModelSaver(save_path=models_dir, save_frequency=1)
```

其中，儲存頻率設為 1，也就是說每次更新模型參數就會儲存一次模型。每模擬完一個批次就會更新一次模型參數。

# `simulation_episodes.py`

```python
import tensorflow as tf
import numpy as np
from baselines.common.policies import PolicyWithValue
from baselines.common import set_global_seeds
from gym_energyplus.envs import EnergyPlusEnv
from baselines.common.models import mlp
import os
from baselines.trpo_mpi.trpo_mpi import traj_segment_generator
from datetime import datetime
try:
    from mpi4py import MPI
except ImportError:
    MPI = None

def main():
    # 設置隨機種子以確保可重現性
    set_global_seeds(0)
    
    # 創建環境（這裡需要替換為您的 EnergyPlus 環境）
    energyplus_file = None
    model_file = os.path.expanduser("~/sim/test_sim/input/2ZoneDataCenterHVAC_wEconomizer_Temp.idf")
    weather_file = os.path.expanduser("~/sim/test_sim/input/USA_CA_San.Francisco.Intl.AP.724940_TMY3_2015_JuneFirst.epw")
    log_dir = None
    if log_dir is None:
        base_log_dir = os.path.expanduser("~/sim/sim_log/")
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = os.path.join(base_log_dir, current_time)
    env = EnergyPlusEnv(
                energyplus_file=energyplus_file,
                model_file=model_file,
                weather_file=weather_file,
                log_dir=log_dir,
                verbose=False,
                seed=None)
    
    # 載入訓練好的策略
    network=mlp(num_hidden=32, num_layers=2)
    pi_policy_network = network(env.observation_space.shape)
    pi_value_network = network(env.observation_space.shape)
    pi = PolicyWithValue(env.action_space, pi_policy_network, pi_value_network)
    pi.load(os.path.expanduser("~/sim/test_sim/nn"))

    # simulation specifical day config

    simulation_days = 1
    timestep_in_idf = 4 # 每 15 分鐘一步
    steps_per_day = timestep_in_idf*24  # 假設每 15 分鐘一步
    total_steps = simulation_days * steps_per_day
    # 進行一次完整的模擬
    seg_gen = traj_segment_generator(pi, env, total_steps, sim=True)

    result = seg_gen.__next__()
    total_reward = result["rew"].sum()

    print(f"The total reward is {total_reward}")

if __name__ == "__main__":
    main()
```

## 時間步（timestep）和環境步（Env.step()）

### 時間步

在 idf 檔之中，`Timestep` 可以設定一小時 EnergyPlus 要模擬幾步，每模擬一步，EnergyPlus 會更新一次環境狀態，包括環境溫度、濕度、能耗表現等等。

```python
  Version,22.2;

  Timestep,4;

  Building,
    Simple One Zone (Wireframe DXF),  !- Name
    0.0000000E+00,           !- North Axis {deg}
    Suburbs,                 !- Terrain
    .04,                     !- Loads Convergence Tolerance Value {W}
    .004,                    !- Temperature Convergence Tolerance Value {deltaC}
    MinimalShadowing,        !- Solar Distribution
    30,                      !- Maximum Number of Warmup Days
    6;                       !- Minimum Number of Warmup Days
```

上面文件表示設置一小時有 4 步。每模擬一步，E+ 會等代 python 進程計算下一步的動作參數。

注意，這裡的 `Timestep` 與標題的時間步（timestep） 不一樣。`Timestep` 是指 E+ 更新一次環境參數（溫度、濕度、能耗等），而時間步則是 python 進程，接收到環境參數到產出動作參數的過程。

### 環境步（Env.step()）

在強化學習中，環境步通常是指在某一时间点，環境根據代理的动作（action）更新自身的狀態（state），並回傳給代理新的觀察值（observation）、獎勵（reward）和是否结束（done）的過程。

詳細內容可以在 `rl-testbed-for-energyplus/gym_energyplus/envs` 的程式碼中看到。

其中， `energyplus_env.py` 是主要的程式碼。

```python
class EnergyPlusEnv(Env):
		//--省略--
		def step(self, action):
        self.timestep1 += 1
        # Send action to the environment
        if action is not None:
            # baselines 0.1.6 changed action type
            if isinstance(action, np.ndarray) and isinstance(action[0], np.ndarray):
                action = action[0]
            self.ep_model.set_action(
                normalized_action=action,
                framework=self.framework
            )

            if not self.send_action():
                print('EnergyPlusEnv.step(): Failed to send an action. Quitting.')
                observation = (self.observation_space.low + self.observation_space.high) * 0.5
                reward = 0.0
                done = True
                print('EnergyPlusEnv: (quit)')
                return observation, reward, done, {}
        
        # Receive observation from the environment    
        # Note that in our co-simulation environment, the state value of the last time step can not be retrived from EnergyPlus process
        # because EMS framework of EnergyPlus does not allow setting EMS calling point ater the last timestep is completed.
        # To remedy this, we assume set_raw_state() method of each model handle the case raw_state is None.
        raw_state, done = self.receive_observation() # raw_state will be None for for call at total_timestep + 1
        self.ep_model.set_raw_state(raw_state)
        observation = self.ep_model.get_state()
        reward = self.ep_model.compute_reward()
        #print(f"reward:{reward}")
        #print(f"timestep:{self.timestep1}")

        if done:
            print('EnergyPlusEnv: (done)')
        return observation, reward, done, {}
```

雖然方法叫做 `send_action`，但從上述的邏輯来看，这个方法實際上是傳遞代理的動作到環境（EnergyPlusEnv）。也就是說是環境在接收動作。